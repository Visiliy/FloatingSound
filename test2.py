import re
import pandas as pd
import numpy as np
import faiss
from pymorphy2 import MorphAnalyzer
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException
from pathlib import Path
import pickle

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
MODEL_NAME = 'distiluse-base-multilingual-cased-v2'  # –ù–æ–≤–∞—è –º–æ–¥–µ–ª—å
DATA_FILE = 'songs2.txt'
INDEX_FILE = 'faiss_index.bin'
EMBEDDINGS_FILE = 'embeddings.pkl'
METADATA_FILE = 'metadata.pkl'

# --- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ---
morph = MorphAnalyzer()

def preprocess(text):
    if pd.isna(text):
        return ''
    text = str(text).lower()
    # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'[^–∞-—è—ë\s.,!?]', '', text)
    words = [morph.parse(word)[0].normal_form for word in text.split()]
    return ' '.join(words)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã ---
def load_or_create_data():
    if Path(INDEX_FILE).exists() and Path(EMBEDDINGS_FILE).exists() and Path(METADATA_FILE).exists():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        index = faiss.read_index(INDEX_FILE)
        with open(EMBEDDINGS_FILE, 'rb') as f:
            embeddings = pickle.load(f)
        with open(METADATA_FILE, 'rb') as f:
            metadata = pickle.load(f)
        return index, embeddings, metadata['titles'], metadata['texts']
    
    print("–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –±–∞–∑—É...")
    df = pd.read_csv(DATA_FILE, sep='|', header=None, names=['title', 'text'])
    df = df.drop_duplicates()  # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    texts = df['text'].tolist()
    titles = df['title'].tolist()
    processed_texts = [preprocess(text) for text in texts]
    
    model = SentenceTransformer(MODEL_NAME)
    embeddings = model.encode(processed_texts, show_progress_bar=True)
    embeddings = np.array(embeddings).astype('float32')
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = embeddings / np.linalg.norm(embeddings, axis=1)[:, np.newaxis]
    
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    faiss.write_index(index, INDEX_FILE)
    with open(EMBEDDINGS_FILE, 'wb') as f:
        pickle.dump(embeddings, f)
    with open(METADATA_FILE, 'wb') as f:
        pickle.dump({'titles': titles, 'texts': texts}, f)
    
    return index, embeddings, titles, texts

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
index, embeddings, titles, texts = load_or_create_data()
model = SentenceTransformer(MODEL_NAME)

# --- –ü–æ–∏—Å–∫ ---
def search(query: str, top_k: int = 5) -> list[dict]:
    try:
        processed_query = preprocess(query)
        query_embedding = model.encode([processed_query])[0].astype('float32')
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
        
        distances, indices = index.search(query_embedding.reshape(1, -1), top_k)
        
        results = []
        for i in range(top_k):
            results.append({
                'title': titles[indices[0][i]],
                'text': texts[indices[0][i]],
                'similarity': float(distances[0][i])
            })
        return results
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
        return []

# --- –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ---
query = input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: ")
results = search(query, 1)
for res in results:
    print(f"\nüéµ {res['title']} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {res['similarity']:.2f})")
    print(f"üìù {res['text'][:200]}...\n")
