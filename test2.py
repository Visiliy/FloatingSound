import re
import pandas as pd
import numpy as np
import faiss
from pymorphy2 import MorphAnalyzer
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer
from fastapi import FastAPI, HTTPException
from pathlib import Path
import pickle

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'
DATA_FILE = 'songs2.txt'
INDEX_FILE = 'faiss_index.bin'
EMBEDDINGS_FILE = 'embeddings.pkl'
METADATA_FILE = 'metadata.pkl'

# --- –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ---
morph = MorphAnalyzer()
stop_words = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', 
    '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ',
    '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç',
    '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ',
    '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', 
    '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', 
    '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å',
    '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±',
    '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', 
    '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º',
    '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ',
    '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏',
    '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ',
    '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ',
    '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π',
    '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º',
    '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
}

def preprocess(text):
    if pd.isna(text):
        return ''
    text = str(text).lower()
    text = re.sub(r'[^–∞-—è—ë\s]', '', text)
    words = [morph.parse(word)[0].normal_form for word in text.split() if word not in stop_words]
    return ' '.join(words)

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã ---
def load_or_create_data():
    if Path(INDEX_FILE).exists() and Path(EMBEDDINGS_FILE).exists() and Path(METADATA_FILE).exists():
        print("–ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        index = faiss.read_index(INDEX_FILE)
        with open(EMBEDDINGS_FILE, 'rb') as f:
            embeddings = pickle.load(f)
        with open(METADATA_FILE, 'rb') as f:
            metadata = pickle.load(f)
        return index, embeddings, metadata['titles'], metadata['texts']
    
    print("–°–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –±–∞–∑—É...")
    df = pd.read_csv(DATA_FILE, sep='|', header=None, names=['title', 'text'])
    texts = df['text'].tolist()
    titles = df['title'].tolist()
    processed_texts = [preprocess(text) for text in texts]
    
    model = SentenceTransformer(MODEL_NAME)
    embeddings = model.encode(processed_texts, show_progress_bar=True)
    embeddings = np.array(embeddings).astype('float32')
    
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    faiss.write_index(index, INDEX_FILE)
    with open(EMBEDDINGS_FILE, 'wb') as f:
        pickle.dump(embeddings, f)
    with open(METADATA_FILE, 'wb') as f:
        pickle.dump({'titles': titles, 'texts': texts}, f)
    
    return index, embeddings, titles, texts

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
index, embeddings, titles, texts = load_or_create_data()
model = SentenceTransformer(MODEL_NAME)

# --- –ü–æ–∏—Å–∫ ---
def search(query: str, top_k: int = 5) -> list[dict]:
    try:
        processed_query = preprocess(query)
        query_embedding = model.encode([processed_query])[0].astype('float32').reshape(1, -1)
        distances, indices = index.search(query_embedding, top_k)
        
        results = []
        for i in range(top_k):
            results.append({
                'title': titles[indices[0][i]],
                'text': texts[indices[0][i]],
                'similarity': float(distances[0][i])  # –î–ª—è JSON-—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
            })
        return results
    except Exception as e:
        return False


query = input("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å: ")
results = search(query, 1)
for res in results:
    print(f"\nüéµ {res['title']} (—Å—Ö–æ–¥—Å—Ç–≤–æ: {res['similarity']:.2f})")
    print(f"üìù {res['text'][:200]}...\n")